{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1961,
     "status": "ok",
     "timestamp": 1568045061864,
     "user": {
      "displayName": "John Smith",
      "photoUrl": "",
      "userId": "08039447143024524011"
     },
     "user_tz": 420
    },
    "id": "h737opJkhZJJ",
    "outputId": "4505005b-62ec-4e16-cbed-213e977ff708"
   },
   "outputs": [],
   "source": [
    "# Clone the entire repo.\n",
    "!git clone -l -s git://github.com/aaron-xichen/pytorch-playground.git cloned-repo\n",
    "%cd cloned-repo\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23719,
     "status": "ok",
     "timestamp": 1568045083644,
     "user": {
      "displayName": "John Smith",
      "photoUrl": "",
      "userId": "08039447143024524011"
     },
     "user_tz": 420
    },
    "id": "tCC87B0TvZ4M",
    "outputId": "a05d7d73-8ea0-4c1d-b2aa-37cfa9b8f0ec"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24664,
     "status": "ok",
     "timestamp": 1568045084598,
     "user": {
      "displayName": "John Smith",
      "photoUrl": "",
      "userId": "08039447143024524011"
     },
     "user_tz": 420
    },
    "id": "FvdUIGEDCqvX",
    "outputId": "90abce05-547c-44e2-95d4-a16d55d9b282"
   },
   "outputs": [],
   "source": [
    "!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/*.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qcyeaPJDDk7e"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9654,
     "status": "error",
     "timestamp": 1567727748307,
     "user": {
      "displayName": "Zefyr Scott",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAsKNamNKOPyFYZGxBguSuTQNUIwA0yEDpRt9kO=s64",
      "userId": "01638418957660869019"
     },
     "user_tz": 420
    },
    "id": "t66h9qBSDYXi",
    "outputId": "82321da7-3caf-4f75-c29f-72fc67b9e111"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jul  9 17:49:00 2019\n",
    "\n",
    "@author: Zefyr Scott\n",
    "\n",
    "Summary of files:\n",
    "    * manager (you are here): Run everything from here. See below for a variety of\n",
    "        configurable settings.\n",
    "    * datasets: Dataset loading. Add additional datasets here, and note them in the\n",
    "        AVAILABLE DATASET LIST in testmanager (below).\n",
    "    * testandtrain: Handles training and testing processes\n",
    "    * nnsupport: The model itself, as well as custom layers and supporting functions\n",
    "    * wideresnet: A classifier called by nnsu\n",
    "    \n",
    "\"\"\"\n",
    "import testandtrain\n",
    "import torch\n",
    "#import torchvision\n",
    "#import torchvision.transforms as transforms\n",
    "import supermodel\n",
    "from datetime import datetime\n",
    "import datasets\n",
    "import tools\n",
    "import smtplib\n",
    "\n",
    "\n",
    "\n",
    "# Settings for various testing and troubleshooting\n",
    "# Setting break_after_first = True runs only one batch, through training only.\n",
    "break_after_first = False\n",
    "# Setting train_only = True runs the training process only.\n",
    "train_only = False\n",
    "# Setting classify_only = True will skip the encoding/decoding steps\n",
    "classify_only = False\n",
    "# Setting print_hooks = True will print various info about gradients, weights,\n",
    "# and biases\n",
    "print_hooks = False\n",
    "# Set this to true if using a Jupyter notebook\n",
    "using_notebook = True\n",
    "print_status = False\n",
    "\n",
    "batch_size_train = 100\n",
    "batch_size_test = 1000\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9 # used for SGD\n",
    "\n",
    "### BIT SIZE STUFF ###\n",
    "# Size of the encoder output. Use as number of bits if fitting all encoder data\n",
    "# to [0,1]\n",
    "encoded_size = 8\n",
    "\n",
    "# Neuron info\n",
    "# * MNIST \"Simple 1D\" encoder: around 200 is a good encoder hidden size.\n",
    "# * CIFAR10 \"FC for RGB\" encoder: around 7000 is performing less badly than others\n",
    "encoder_hidden_size = 200\n",
    "classifier_hidden_size = 1000\n",
    "\n",
    "# Dataset management\n",
    "# AVAILABLE DATASET LIST: MNIST, CIFAR10\n",
    "dataset = \"CIFAR10\"\n",
    "\n",
    "# Splitters for different datasets:\n",
    "# MNIST: 1D\n",
    "# CIFAR10: FC for RGB, Color Separation\n",
    "#   * FC for RGB: used with Simple 1D encoder and decoder\n",
    "#   * Color Separation: used with Simple Conv encoder and decoder, and\n",
    "#     num_of_encoders = 3. This handles each color layer as a distinct 2d image\n",
    "#     for encoding and decoding\n",
    "#   * 2D: splits along two different dimensions\n",
    "splitter = \"Color Separation\"\n",
    "\n",
    "# Encoders for different datasets:\n",
    "# MNIST: Simple 1D\n",
    "# CIFAR10: Simple 1D, 1 Channel Conv, 1 Channel Conv Square, AlexNetEncode, ResNetEncode\n",
    "encoder = \"ResNetEncode\"\n",
    "\n",
    "# Decoders for different datasets:\n",
    "# MNIST: Simple 1D\n",
    "# CIFAR10: Simple 1D, 1 to 3 Channel Conv, 1 to 3 Channel Conv Square\n",
    "decoder = \"1 to 3 Channel Conv\"\n",
    "\n",
    "# Classifiers for different datasets:\n",
    "# Pretrained classifiers are from https://github.com/aaron-xichen/pytorch-playground\n",
    "# MNIST: MNIST Tutorial, Pretrained\n",
    "# CIFAR10: CIFAR10 Tutorial, Pretrained\n",
    "classifier = \"Pretrained\"\n",
    "\n",
    "# Load stuff for this dataset\n",
    "loaders, classes, unencoded_dims, pretrained_classifier = datasets.loadDataset(dataset, encoder, classifier, batch_size_train, batch_size_test)\n",
    "\n",
    "## For splitting along one dimension only:\n",
    "## Set num_of_encoders to an integer to split the input automatically as evenly as\n",
    "## possible. Alternately, if manual splitting is desired, ignore the num_of_encoders\n",
    "## and instead directly assign a tuple to subsets indicating the desired splitting,\n",
    "## ie if input size is 15 the desired splitting might be subsets = (8, 4, 3)\n",
    "num_of_encoders = 3\n",
    "subsets = tools.split_sizes(unencoded_dims['size'], num_of_encoders)\n",
    "dims = 1\n",
    "# For splitting across two different dimensions: if 4d RGB and splitting image with\n",
    "# intact colors into even chunks, just specify the number of rows and columns\n",
    "#rows = 2\n",
    "#cols = 2\n",
    "#subsets = (tools.split_sizes(unencoded_dims['height'], rows), tools.split_sizes(unencoded_dims['width'], cols))\n",
    "#dims = (2,3)\n",
    "\n",
    "# Loss function management\n",
    "# AVAILABLE LOSS FUNCTION LIST: CrossEntropyLoss, CELWithEncodedMSE\n",
    "loss_function = \"CrossEntropyLoss\"\n",
    "#If using CELWithEncodedMSE, multiply the MSE part by this constant\n",
    "mse_multiplier = 1\n",
    "\n",
    "# General handling stuff\n",
    "outfile = '/content/gdrive/My Drive/Colab Notebooks/{}.txt'.format(datetime.strftime(datetime.now(), \"%y%m%d-%H%M%S\"))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = \"cpu\"\n",
    "\n",
    "#for i  in learning_rate_set:\n",
    "#for i in range(1,5):\n",
    "    #resead at start of each run for repeatability\n",
    "torch.manual_seed(1)    \n",
    "#outfile = None\n",
    "\n",
    "with open(outfile, 'a') as outf:\n",
    "    print(datetime.now().time(), 'Dataset: ', dataset, file=outf)\n",
    "    print(datetime.now().time(), 'Classifier: ', classifier, file=outf)\n",
    "    print(datetime.now().time(), 'Epochs: ', num_epochs, file=outf)\n",
    "    print(datetime.now().time(), 'Learning rate: ', learning_rate, file=outf)\n",
    "    print(datetime.now().time(), 'Loss function: ', loss_function, file=outf)\n",
    "    print(datetime.now().time(), 'Optimizer: Adam', file=outf)\n",
    "    if (classify_only):\n",
    "        print(datetime.now().time(), 'Classification test only ', file=outf)\n",
    "    else:\n",
    "        if (encoder == '3 Channel Small Conv'):\n",
    "            print(datetime.now().time(), 'Encoders: ', rows * cols, file=outf)\n",
    "        else: print(datetime.now().time(), 'Encoders: ', num_of_encoders, file=outf)\n",
    "        print(datetime.now().time(), 'Encoder type: ', encoder, file=outf)\n",
    "        if (encoder == 'Simple 1D'):\n",
    "            print(datetime.now().time(), 'Encoder neurons: ', encoder_hidden_size, file=outf)\n",
    "        print(datetime.now().time(), 'Bits per encoder: ', encoded_size, file=outf)\n",
    "        print(datetime.now().time(), 'Decoder type: ', decoder, file=outf)\n",
    "        \n",
    "unrounded_error, rounded_error = testandtrain.testandtrain(loaders,\n",
    "            num_epochs,\n",
    "            learning_rate,\n",
    "            momentum,\n",
    "            unencoded_dims,\n",
    "            subsets,\n",
    "            dims,\n",
    "            mse_multiplier,\n",
    "            encoded_size,\n",
    "            classes,\n",
    "            encoder_hidden_size,\n",
    "            classifier_hidden_size,\n",
    "            loss_function,\n",
    "            splitter,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            classifier,\n",
    "            outfile,\n",
    "            break_after_first,\n",
    "            train_only,\n",
    "            classify_only, \n",
    "            print_hooks,\n",
    "            using_notebook,\n",
    "            device,\n",
    "            pretrained_classifier,\n",
    "            print_status)\n",
    "\n",
    "with open(outfile, 'a') as outf:\n",
    "    print(datetime.now().time(), 'Unrounded error: ', unrounded_error, '\\n', file=outf)\n",
    "    print(datetime.now().time(), 'Rounded error: ', rounded_error, '\\n', file=outf)\n",
    "\n",
    "# Stuff can take a long time to run so alert me when done\n",
    "tools.alert()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "manager.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
