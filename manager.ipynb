{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"manager.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"h737opJkhZJJ","colab_type":"code","outputId":"4505005b-62ec-4e16-cbed-213e977ff708","executionInfo":{"status":"ok","timestamp":1568045061864,"user_tz":420,"elapsed":1961,"user":{"displayName":"John Smith","photoUrl":"","userId":"08039447143024524011"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Clone the entire repo.\n","!git clone -l -s git://github.com/aaron-xichen/pytorch-playground.git cloned-repo\n","%cd cloned-repo\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'cloned-repo'...\n","warning: --local is ignored\n","remote: Enumerating objects: 114, done.\u001b[K\n","remote: Total 114 (delta 0), reused 0 (delta 0), pack-reused 114\u001b[K\n","Receiving objects:   0% (1/114)   \rReceiving objects:   1% (2/114)   \rReceiving objects:   2% (3/114)   \rReceiving objects:   3% (4/114)   \rReceiving objects:   4% (5/114)   \rReceiving objects:   5% (6/114)   \rReceiving objects:   6% (7/114)   \rReceiving objects:   7% (8/114)   \rReceiving objects:   8% (10/114)   \rReceiving objects:   9% (11/114)   \rReceiving objects:  10% (12/114)   \rReceiving objects:  11% (13/114)   \rReceiving objects:  12% (14/114)   \rReceiving objects:  13% (15/114)   \rReceiving objects:  14% (16/114)   \rReceiving objects:  15% (18/114)   \rReceiving objects:  16% (19/114)   \rReceiving objects:  17% (20/114)   \rReceiving objects:  18% (21/114)   \rReceiving objects:  19% (22/114)   \rReceiving objects:  20% (23/114)   \rReceiving objects:  21% (24/114)   \rReceiving objects:  22% (26/114)   \rReceiving objects:  23% (27/114)   \rReceiving objects:  24% (28/114)   \rReceiving objects:  25% (29/114)   \rReceiving objects:  26% (30/114)   \rReceiving objects:  27% (31/114)   \rReceiving objects:  28% (32/114)   \rReceiving objects:  29% (34/114)   \rReceiving objects:  30% (35/114)   \rReceiving objects:  31% (36/114)   \rReceiving objects:  32% (37/114)   \rReceiving objects:  33% (38/114)   \rReceiving objects:  34% (39/114)   \rReceiving objects:  35% (40/114)   \rReceiving objects:  36% (42/114)   \rReceiving objects:  37% (43/114)   \rReceiving objects:  38% (44/114)   \rReceiving objects:  39% (45/114)   \rReceiving objects:  40% (46/114)   \rReceiving objects:  41% (47/114)   \rReceiving objects:  42% (48/114)   \rReceiving objects:  43% (50/114)   \rReceiving objects:  44% (51/114)   \rReceiving objects:  45% (52/114)   \rReceiving objects:  46% (53/114)   \rReceiving objects:  47% (54/114)   \rReceiving objects:  48% (55/114)   \rReceiving objects:  49% (56/114)   \rReceiving objects:  50% (57/114)   \rReceiving objects:  51% (59/114)   \rReceiving objects:  52% (60/114)   \rReceiving objects:  53% (61/114)   \rReceiving objects:  54% (62/114)   \rReceiving objects:  55% (63/114)   \rReceiving objects:  56% (64/114)   \rReceiving objects:  57% (65/114)   \rReceiving objects:  58% (67/114)   \rReceiving objects:  59% (68/114)   \rReceiving objects:  60% (69/114)   \rReceiving objects:  61% (70/114)   \rReceiving objects:  62% (71/114)   \rReceiving objects:  63% (72/114)   \rReceiving objects:  64% (73/114)   \rReceiving objects:  65% (75/114)   \rReceiving objects:  66% (76/114)   \rReceiving objects:  67% (77/114)   \rReceiving objects:  68% (78/114)   \rReceiving objects:  69% (79/114)   \rReceiving objects:  70% (80/114)   \rReceiving objects:  71% (81/114)   \rReceiving objects:  72% (83/114)   \rReceiving objects:  73% (84/114)   \rReceiving objects:  74% (85/114)   \rReceiving objects:  75% (86/114)   \rReceiving objects:  76% (87/114)   \rReceiving objects:  77% (88/114)   \rReceiving objects:  78% (89/114)   \rReceiving objects:  79% (91/114)   \rReceiving objects:  80% (92/114)   \rReceiving objects:  81% (93/114)   \rReceiving objects:  82% (94/114)   \rReceiving objects:  83% (95/114)   \rReceiving objects:  84% (96/114)   \rReceiving objects:  85% (97/114)   \rReceiving objects:  86% (99/114)   \rReceiving objects:  87% (100/114)   \rReceiving objects:  88% (101/114)   \rReceiving objects:  89% (102/114)   \rReceiving objects:  90% (103/114)   \rReceiving objects:  91% (104/114)   \rReceiving objects:  92% (105/114)   \rReceiving objects:  93% (107/114)   \rReceiving objects:  94% (108/114)   \rReceiving objects:  95% (109/114)   \rReceiving objects:  96% (110/114)   \rReceiving objects:  97% (111/114)   \rReceiving objects:  98% (112/114)   \rReceiving objects:  99% (113/114)   \rReceiving objects: 100% (114/114)   \rReceiving objects: 100% (114/114), 38.43 KiB | 9.61 MiB/s, done.\n","Resolving deltas:   0% (0/63)   \rResolving deltas:   1% (1/63)   \rResolving deltas:   9% (6/63)   \rResolving deltas:  23% (15/63)   \rResolving deltas:  26% (17/63)   \rResolving deltas:  34% (22/63)   \rResolving deltas:  41% (26/63)   \rResolving deltas:  49% (31/63)   \rResolving deltas:  53% (34/63)   \rResolving deltas:  55% (35/63)   \rResolving deltas:  58% (37/63)   \rResolving deltas:  60% (38/63)   \rResolving deltas:  63% (40/63)   \rResolving deltas:  68% (43/63)   \rResolving deltas:  69% (44/63)   \rResolving deltas:  74% (47/63)   \rResolving deltas:  76% (48/63)   \rResolving deltas:  80% (51/63)   \rResolving deltas:  85% (54/63)   \rResolving deltas:  87% (55/63)   \rResolving deltas:  88% (56/63)   \rResolving deltas:  96% (61/63)   \rResolving deltas:  98% (62/63)   \rResolving deltas: 100% (63/63)   \rResolving deltas: 100% (63/63), done.\n","/content/cloned-repo\n","cifar\t  LICENSE  quantize.py\troadmap_zh.md  stl10  utee\n","imagenet  mnist    README.md\tscript\t       svhn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tCC87B0TvZ4M","colab_type":"code","outputId":"a05d7d73-8ea0-4c1d-b2aa-37cfa9b8f0ec","executionInfo":{"status":"ok","timestamp":1568045083644,"user_tz":420,"elapsed":23719,"user":{"displayName":"John Smith","photoUrl":"","userId":"08039447143024524011"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FvdUIGEDCqvX","colab_type":"code","outputId":"90abce05-547c-44e2-95d4-a16d55d9b282","executionInfo":{"status":"ok","timestamp":1568045084598,"user_tz":420,"elapsed":24664,"user":{"displayName":"John Smith","photoUrl":"","userId":"08039447143024524011"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["!ls /content/gdrive/My\\ Drive/Colab\\ Notebooks/*.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'/content/gdrive/My Drive/Colab Notebooks/datasets.py'\n","'/content/gdrive/My Drive/Colab Notebooks/__init__.py'\n","'/content/gdrive/My Drive/Colab Notebooks/supermodel.py'\n","'/content/gdrive/My Drive/Colab Notebooks/testandtrain.py'\n","'/content/gdrive/My Drive/Colab Notebooks/tools.py'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qcyeaPJDDk7e","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append('/content/gdrive/My Drive/Colab Notebooks')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t66h9qBSDYXi","colab_type":"code","outputId":"82321da7-3caf-4f75-c29f-72fc67b9e111","executionInfo":{"status":"error","timestamp":1567727748307,"user_tz":420,"elapsed":9654,"user":{"displayName":"Zefyr Scott","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAsKNamNKOPyFYZGxBguSuTQNUIwA0yEDpRt9kO=s64","userId":"01638418957660869019"}},"colab":{"base_uri":"https://localhost:8080/","height":17}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Jul  9 17:49:00 2019\n","\n","@author: Zefyr Scott\n","\n","Summary of files:\n","    * manager (you are here): Run everything from here. See below for a variety of\n","        configurable settings.\n","    * datasets: Dataset loading. Add additional datasets here, and note them in the\n","        AVAILABLE DATASET LIST in testmanager (below).\n","    * testandtrain: Handles training and testing processes\n","    * nnsupport: The model itself, as well as custom layers and supporting functions\n","    * wideresnet: A classifier called by nnsu\n","    \n","\"\"\"\n","import testandtrain\n","import torch\n","#import torchvision\n","#import torchvision.transforms as transforms\n","import supermodel\n","from datetime import datetime\n","import datasets\n","import tools\n","import smtplib\n","\n","\n","\n","# Settings for various testing and troubleshooting\n","# Setting break_after_first = True runs only one batch, through training only.\n","break_after_first = False\n","# Setting train_only = True runs the training process only.\n","train_only = False\n","# Setting classify_only = True will skip the encoding/decoding steps\n","classify_only = False\n","# Setting print_hooks = True will print various info about gradients, weights,\n","# and biases\n","print_hooks = False\n","# Set this to true if using a Jupyter notebook\n","using_notebook = True\n","print_status = False\n","\n","batch_size_train = 100\n","batch_size_test = 1000\n","\n","num_epochs = 50\n","learning_rate = 0.001\n","momentum = 0.9 # used for SGD\n","\n","### BIT SIZE STUFF ###\n","# Size of the encoder output. Use as number of bits if fitting all encoder data\n","# to [0,1]\n","encoded_size = 8\n","\n","# Neuron info\n","# * MNIST \"Simple 1D\" encoder: around 200 is a good encoder hidden size.\n","# * CIFAR10 \"FC for RGB\" encoder: around 7000 is performing less badly than others\n","encoder_hidden_size = 200\n","classifier_hidden_size = 1000\n","\n","# Dataset management\n","# AVAILABLE DATASET LIST: MNIST, CIFAR10\n","dataset = \"CIFAR10\"\n","\n","# Splitters for different datasets:\n","# MNIST: 1D\n","# CIFAR10: FC for RGB, Color Separation\n","#   * FC for RGB: used with Simple 1D encoder and decoder\n","#   * Color Separation: used with Simple Conv encoder and decoder, and\n","#     num_of_encoders = 3. This handles each color layer as a distinct 2d image\n","#     for encoding and decoding\n","#   * 2D: splits along two different dimensions\n","splitter = \"Color Separation\"\n","\n","# Encoders for different datasets:\n","# MNIST: Simple 1D\n","# CIFAR10: Simple 1D, 1 Channel Conv, 1 Channel Conv Square, AlexNetEncode, ResNetEncode\n","encoder = \"ResNetEncode\"\n","\n","# Decoders for different datasets:\n","# MNIST: Simple 1D\n","# CIFAR10: Simple 1D, 1 to 3 Channel Conv, 1 to 3 Channel Conv Square\n","decoder = \"1 to 3 Channel Conv\"\n","\n","# Classifiers for different datasets:\n","# Pretrained classifiers are from https://github.com/aaron-xichen/pytorch-playground\n","# MNIST: MNIST Tutorial, Pretrained\n","# CIFAR10: CIFAR10 Tutorial, Pretrained\n","classifier = \"Pretrained\"\n","\n","# Load stuff for this dataset\n","loaders, classes, unencoded_dims, pretrained_classifier = datasets.loadDataset(dataset, encoder, classifier, batch_size_train, batch_size_test)\n","\n","## For splitting along one dimension only:\n","## Set num_of_encoders to an integer to split the input automatically as evenly as\n","## possible. Alternately, if manual splitting is desired, ignore the num_of_encoders\n","## and instead directly assign a tuple to subsets indicating the desired splitting,\n","## ie if input size is 15 the desired splitting might be subsets = (8, 4, 3)\n","num_of_encoders = 3\n","subsets = tools.split_sizes(unencoded_dims['size'], num_of_encoders)\n","dims = 1\n","# For splitting across two different dimensions: if 4d RGB and splitting image with\n","# intact colors into even chunks, just specify the number of rows and columns\n","#rows = 2\n","#cols = 2\n","#subsets = (tools.split_sizes(unencoded_dims['height'], rows), tools.split_sizes(unencoded_dims['width'], cols))\n","#dims = (2,3)\n","\n","# Loss function management\n","# AVAILABLE LOSS FUNCTION LIST: CrossEntropyLoss, CELWithEncodedMSE\n","loss_function = \"CrossEntropyLoss\"\n","#If using CELWithEncodedMSE, multiply the MSE part by this constant\n","mse_multiplier = 1\n","\n","# General handling stuff\n","outfile = '/content/gdrive/My Drive/Colab Notebooks/{}.txt'.format(datetime.strftime(datetime.now(), \"%y%m%d-%H%M%S\"))\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","#device = \"cpu\"\n","\n","#for i  in learning_rate_set:\n","#for i in range(1,5):\n","    #resead at start of each run for repeatability\n","torch.manual_seed(1)    \n","#outfile = None\n","\n","with open(outfile, 'a') as outf:\n","    print(datetime.now().time(), 'Dataset: ', dataset, file=outf)\n","    print(datetime.now().time(), 'Classifier: ', classifier, file=outf)\n","    print(datetime.now().time(), 'Epochs: ', num_epochs, file=outf)\n","    print(datetime.now().time(), 'Learning rate: ', learning_rate, file=outf)\n","    print(datetime.now().time(), 'Loss function: ', loss_function, file=outf)\n","    print(datetime.now().time(), 'Optimizer: Adam', file=outf)\n","    if (classify_only):\n","        print(datetime.now().time(), 'Classification test only ', file=outf)\n","    else:\n","        if (encoder == '3 Channel Small Conv'):\n","            print(datetime.now().time(), 'Encoders: ', rows * cols, file=outf)\n","        else: print(datetime.now().time(), 'Encoders: ', num_of_encoders, file=outf)\n","        print(datetime.now().time(), 'Encoder type: ', encoder, file=outf)\n","        if (encoder == 'Simple 1D'):\n","            print(datetime.now().time(), 'Encoder neurons: ', encoder_hidden_size, file=outf)\n","        print(datetime.now().time(), 'Bits per encoder: ', encoded_size, file=outf)\n","        print(datetime.now().time(), 'Decoder type: ', decoder, file=outf)\n","        \n","unrounded_error, rounded_error = testandtrain.testandtrain(loaders,\n","            num_epochs,\n","            learning_rate,\n","            momentum,\n","            unencoded_dims,\n","            subsets,\n","            dims,\n","            mse_multiplier,\n","            encoded_size,\n","            classes,\n","            encoder_hidden_size,\n","            classifier_hidden_size,\n","            loss_function,\n","            splitter,\n","            encoder,\n","            decoder,\n","            classifier,\n","            outfile,\n","            break_after_first,\n","            train_only,\n","            classify_only, \n","            print_hooks,\n","            using_notebook,\n","            device,\n","            pretrained_classifier,\n","            print_status)\n","\n","with open(outfile, 'a') as outf:\n","    print(datetime.now().time(), 'Unrounded error: ', unrounded_error, '\\n', file=outf)\n","    print(datetime.now().time(), 'Rounded error: ', rounded_error, '\\n', file=outf)\n","\n","# Stuff can take a long time to run so alert me when done\n","tools.alert()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\r0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"],"name":"stdout"},{"output_type":"stream","text":[" 89%|████████▉ | 151789568/170498071 [00:02<00:00, 70843129.83it/s]"],"name":"stderr"}]}]}